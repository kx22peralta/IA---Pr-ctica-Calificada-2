{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 623 µs (started: 2021-06-12 15:58:34 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# medir tiempos\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark [Apache spark](https://spark.apache.org/)\n",
    "Apache Spark™ es un motor de análisis unificado para el procesamiento de datos a gran escala.\n",
    "### pyspark\n",
    "PySpark es una interfaz para Apache Spark en Python. No solo le permite escribir aplicaciones Spark utilizando las API de Python, sino que también proporciona el shell de PySpark para analizar interactivamente sus datos en un entorno distribuido. PySpark es compatible con la mayoría de las funciones de Spark, como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) y Spark Core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modulo para encontrar pyspark en el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.12 ms (started: 2021-06-12 15:58:36 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/spark-3.1.1-bin-hadoop2.7\")    #para linux\n",
    "#findspark.init()                                                 #para windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ModeloML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=ModeloML>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.82 s (started: 2021-06-12 15:58:36 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# Variable de configuración\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"ModeloML\").set(\"spark.driver.maxResultSize\",\"0\")\n",
    "# iniciamos un contexto spark (solo se ejecuta uno. Para ejecutar otra vez , reiniciar el kernel)\n",
    "sc = SparkContext(conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 21s (started: 2021-06-12 15:58:41 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "# le pasamos el contexto anterior\n",
    "sqlContext = SQLContext(sc)\n",
    "dfspark = sqlContext.read.format('csv').option(\"header\",\"true\").option(\"inferSchema\",\"true\").load('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columnas y tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n",
      "time: 24.2 ms (started: 2021-06-12 16:00:03 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de filas del conjunto de datos 55423856\n",
      "time: 14.7 s (started: 2021-06-12 16:00:03 -05:00)\n"
     ]
    }
   ],
   "source": [
    "cant_total=dfspark.count()\n",
    "print(\"total de filas del conjunto de datos\",cant_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tomando una muestra del 4% del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.34 ms (started: 2021-06-12 16:00:18 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample = dfspark.sample(fraction = 0.04, withReplacement = False, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total de filas de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de filas de la muestra del conjunto de datos 2219408\n",
      "time: 16.4 s (started: 2021-06-12 16:00:18 -05:00)\n"
     ]
    }
   ],
   "source": [
    "cant_muestra=dfspark_sample.count()\n",
    "print(\"total de filas de la muestra del conjunto de datos\",cant_muestra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de la muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procederá a eliminar la columna con la característica \"key\", debido a que contiene datos innecesarios para lograr el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.9 ms (started: 2021-06-12 16:00:34 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample = dfspark_sample.drop('key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminando Valores Nulos de la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 446 ms (started: 2021-06-12 16:00:34 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# fare_amount (costo de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount is not NULL\")\n",
    "# passenger (número de pasajeros) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"passenger_count is not NULL\")\n",
    "# pickup_datetime (fecha y hora de incio de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_datetime is not NULL\")\n",
    "# pickup (longitud y latitud de inicio de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_latitude is not NULL\")\n",
    "# dropoff (longitud y laitud de fin de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_latitude is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminado valores nan y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas eliminadas de la muestra:  18\n",
      "time: 1min 6s (started: 2021-06-12 16:00:35 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# tabla sin valores nan, sin duplicados\n",
    "dfspark_sample=dfspark_sample.na.drop().dropDuplicates()\n",
    "# cantidad de  data sin nulos ni nan\n",
    "cantnn_muestra=dfspark_sample.count()\n",
    "print(\"Cantidad de filas eliminadas de la muestra: \",cant_muestra-cantnn_muestra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La muestra se guarda en el HDFS \n",
    "Este metodo acelera algunos procesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fare_amount: double, pickup_datetime: string, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, passenger_count: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 135 ms (started: 2021-06-12 16:01:41 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadisticas de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 220 ms (started: 2021-06-12 16:01:41 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 7s (started: 2021-06-12 16:01:41 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# casteamos a pandas\n",
    "estadisticas=dfspark_sample.describe([\"pickup_longitude\",\n",
    "                                 \"pickup_latitude\",\n",
    "                                 \"dropoff_longitude\",\n",
    "                                 \"dropoff_latitude\",\n",
    "                                 \"passenger_count\",\n",
    "                                 \"fare_amount\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>-72.50355231460159</td>\n",
       "      <td>39.91779369363991</td>\n",
       "      <td>-72.50697030839864</td>\n",
       "      <td>39.91850726486102</td>\n",
       "      <td>1.6851959322156087</td>\n",
       "      <td>11.352503967306385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>13.098889190132796</td>\n",
       "      <td>9.821243657749829</td>\n",
       "      <td>13.244549098861283</td>\n",
       "      <td>10.317917931047642</td>\n",
       "      <td>1.3377753769552685</td>\n",
       "      <td>42.449025079080954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-3366.527908</td>\n",
       "      <td>-3488.079513</td>\n",
       "      <td>-3366.527908</td>\n",
       "      <td>-3488.079513</td>\n",
       "      <td>0</td>\n",
       "      <td>-52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>2497.117435</td>\n",
       "      <td>2964.163855</td>\n",
       "      <td>3211.57975</td>\n",
       "      <td>3333.304575</td>\n",
       "      <td>208</td>\n",
       "      <td>61550.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary    pickup_longitude    pickup_latitude   dropoff_longitude  \\\n",
       "0   count             2219390            2219390             2219390   \n",
       "1    mean  -72.50355231460159  39.91779369363991  -72.50697030839864   \n",
       "2  stddev  13.098889190132796  9.821243657749829  13.244549098861283   \n",
       "3     min        -3366.527908       -3488.079513        -3366.527908   \n",
       "4     max         2497.117435        2964.163855          3211.57975   \n",
       "\n",
       "     dropoff_latitude     passenger_count         fare_amount  \n",
       "0             2219390             2219390             2219390  \n",
       "1   39.91850726486102  1.6851959322156087  11.352503967306385  \n",
       "2  10.317917931047642  1.3377753769552685  42.449025079080954  \n",
       "3        -3488.079513                   0               -52.0  \n",
       "4         3333.304575                 208            61550.86  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.4 ms (started: 2021-06-12 16:02:48 -05:00)\n"
     ]
    }
   ],
   "source": [
    "estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cantidad viajes por número de pasajeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|passenger_count|  count|\n",
      "+---------------+-------+\n",
      "|              1|1535882|\n",
      "|              6|  47132|\n",
      "|              3|  97242|\n",
      "|              5| 157254|\n",
      "|              4|  47158|\n",
      "|              7|      1|\n",
      "|              2| 326983|\n",
      "|              0|   7734|\n",
      "|            208|      4|\n",
      "+---------------+-------+\n",
      "\n",
      "time: 3.49 s (started: 2021-06-12 16:02:48 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observaciones :\n",
    "1. Valores de passenger_count imposibles , como 34,49,51,129,208.\n",
    "2. Precios demasiados elevados debido a la cantidad de pasajeros y negativo(imposible).\n",
    "3. Cantidad de datos en la que el precio es menor a o igual a 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### minimos y máximos de latitude y longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|min(pickup_longitude)|\n",
      "+---------------------+\n",
      "|         -3366.527908|\n",
      "+---------------------+\n",
      "\n",
      "+---------------------+\n",
      "|max(pickup_longitude)|\n",
      "+---------------------+\n",
      "|          2497.117435|\n",
      "+---------------------+\n",
      "\n",
      "+----------------------+\n",
      "|min(dropoff_longitude)|\n",
      "+----------------------+\n",
      "|          -3366.527908|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|max(dropoff_longitude)|\n",
      "+----------------------+\n",
      "|            3211.57975|\n",
      "+----------------------+\n",
      "\n",
      "+--------------------+\n",
      "|min(pickup_latitude)|\n",
      "+--------------------+\n",
      "|        -3488.079513|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|max(pickup_latitude)|\n",
      "+--------------------+\n",
      "|         2964.163855|\n",
      "+--------------------+\n",
      "\n",
      "+---------------------+\n",
      "|min(dropoff_latitude)|\n",
      "+---------------------+\n",
      "|         -3488.079513|\n",
      "+---------------------+\n",
      "\n",
      "+---------------------+\n",
      "|max(dropoff_latitude)|\n",
      "+---------------------+\n",
      "|          3333.304575|\n",
      "+---------------------+\n",
      "\n",
      "time: 6.92 s (started: 2021-06-12 16:02:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# mínimo y máximo para longitud del subconjunto de datos\n",
    "long_min_i=dfspark_sample.agg({'pickup_longitude': 'min'}).show()\n",
    "long_max_i=dfspark_sample.agg({'pickup_longitude': 'max'}).show()\n",
    "long_min_f=dfspark_sample.agg({'dropoff_longitude': 'min'}).show()\n",
    "long_max_f=dfspark_sample.agg({'dropoff_longitude': 'max'}).show()\n",
    "\n",
    "# mínimo y máximo para para latitud del subconjuntos de datos\n",
    "lat_min_i=dfspark_sample.agg({'pickup_latitude': 'min'}).show()\n",
    "lat_max_i=dfspark_sample.agg({'pickup_latitude': 'max'}).show()\n",
    "lat_min_f=dfspark_sample.agg({'dropoff_latitude': 'min'}).show()\n",
    "lat_max_f=dfspark_sample.agg({'dropoff_latitude': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observaciones:\n",
    "1. Valores para longitud imposibles, ya que longitud varía entre -90 y 90\n",
    "2. Valores para latitud imposibles, ya que latitud varía entre -180 y 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seleccionar passenger_count de 0-9\n",
    "El número de pasajeros de cada viaje puede ser entre 0 a 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 56.8 ms (started: 2021-06-12 16:02:59 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample = dfspark_sample.filter(\"passenger_count < 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|passenger_count|  count|\n",
      "+---------------+-------+\n",
      "|              1|1535882|\n",
      "|              6|  47132|\n",
      "|              3|  97242|\n",
      "|              5| 157254|\n",
      "|              4|  47158|\n",
      "|              7|      1|\n",
      "|              2| 326983|\n",
      "|              0|   7734|\n",
      "+---------------+-------+\n",
      "\n",
      "time: 3.22 s (started: 2021-06-12 16:02:59 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seleccionar fare_amount mayores o iguales a cero\n",
    "El precio del viaje puede no puede ser menor que cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.1 ms (started: 2021-06-12 16:03:02 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Selecionar fare_amount mayor a 0\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount >= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       fare_amount|\n",
      "+-------+------------------+\n",
      "|  count|           2219298|\n",
      "|   mean|11.353255646605419|\n",
      "| stddev| 42.44968689216035|\n",
      "|    min|               0.0|\n",
      "|    max|          61550.86|\n",
      "+-------+------------------+\n",
      "\n",
      "time: 1.51 s (started: 2021-06-12 16:03:02 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.describe([\"fare_amount\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latitud y Longitud validas.\n",
    "La latitud válida varía entre -90 a 90.\n",
    "La longitud válida varía entre -180 a 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 65.9 ms (started: 2021-06-12 16:03:03 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Filtrando valores grandes para longitud, de tal manera que solo se considerará valores correctos.\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_longitude < 180 and pickup_longitude > -180\" )\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_longitude < 180 and dropoff_longitude > -180\")\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_latitude < 90 and pickup_latitude > -90\" )\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_latitude < 90 and dropoff_latitude > -90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.76 s (started: 2021-06-12 16:03:04 -05:00)\n"
     ]
    }
   ],
   "source": [
    "estadisticas=dfspark_sample.describe([\"pickup_longitude\",\n",
    "                                 \"pickup_latitude\",\n",
    "                                 \"dropoff_longitude\",\n",
    "                                 \"dropoff_latitude\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>2219173</td>\n",
       "      <td>2219173</td>\n",
       "      <td>2219173</td>\n",
       "      <td>2219173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>-72.49600158477304</td>\n",
       "      <td>39.91679455251933</td>\n",
       "      <td>-72.50229931133802</td>\n",
       "      <td>39.91968272022599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>10.463045755203852</td>\n",
       "      <td>6.112153680008393</td>\n",
       "      <td>10.439016533276373</td>\n",
       "      <td>6.104146217099324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-121.91499328613281</td>\n",
       "      <td>-74.017222</td>\n",
       "      <td>-121.9151840209961</td>\n",
       "      <td>-74.177303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>40.840962</td>\n",
       "      <td>74.007413</td>\n",
       "      <td>73.93996</td>\n",
       "      <td>74.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary     pickup_longitude    pickup_latitude   dropoff_longitude  \\\n",
       "0   count              2219173            2219173             2219173   \n",
       "1    mean   -72.49600158477304  39.91679455251933  -72.50229931133802   \n",
       "2  stddev   10.463045755203852  6.112153680008393  10.439016533276373   \n",
       "3     min  -121.91499328613281         -74.017222  -121.9151840209961   \n",
       "4     max            40.840962          74.007413            73.93996   \n",
       "\n",
       "    dropoff_latitude  \n",
       "0            2219173  \n",
       "1  39.91968272022599  \n",
       "2  6.104146217099324  \n",
       "3         -74.177303  \n",
       "4              74.95  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.2 ms (started: 2021-06-12 16:03:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diferencias de latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms (started: 2021-06-12 16:03:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Agregamos columnas de diferencias.\n",
    "from pyspark.sql.functions import abs\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_latitude\",\n",
    "                                           abs(dfspark_sample['dropoff_latitude']-dfspark_sample['pickup_latitude']))\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_longitude\",\n",
    "                                           abs(dfspark_sample['dropoff_longitude']-dfspark_sample['pickup_longitude']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distancia Haversine\n",
    "Es la distancia entre dos puntos geográficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 115 ms (started: 2021-06-12 16:03:07 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Creamos la función para hallar la distancia entre dos puntos geográficos\n",
    "import math\n",
    "from pyspark.sql.functions import udf, array, col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def haversine(x):\n",
    "    lat1=x[0]\n",
    "    lon1=x[1]\n",
    "    lat2=x[2]\n",
    "    lon2=x[3]\n",
    "    \n",
    "    rad=math.pi/180\n",
    "    dlat=lat2-lat1\n",
    "    dlon=lon2-lon1\n",
    "    R=6372.795477598\n",
    "    a=(math.sin(rad*dlat/2))**2 + math.cos(rad*lat1)*math.cos(rad*lat2)*(math.sin(rad*dlon/2))**2\n",
    "    distancia=2*R*math.asin(math.sqrt(a))\n",
    "    return distancia\n",
    "\n",
    "distancia_udf = udf(lambda z: haversine(z), FloatType())\n",
    "#spark.udf.register('distancia_udf', distancia_udf)\n",
    "dfspark_sample = dfspark_sample.withColumn('distancia', distancia_udf(array('pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude')))                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| distancia|\n",
      "+----------+\n",
      "| 1.8683056|\n",
      "| 1.8191968|\n",
      "|  1.145462|\n",
      "| 1.0793539|\n",
      "| 1.1367035|\n",
      "|  2.424119|\n",
      "|  3.829884|\n",
      "| 1.0900514|\n",
      "| 5.5276585|\n",
      "|  3.122764|\n",
      "| 2.3627946|\n",
      "| 5.3667502|\n",
      "| 1.3431213|\n",
      "|  9.317738|\n",
      "|  5.013023|\n",
      "| 4.6022677|\n",
      "| 1.1546545|\n",
      "| 20.964258|\n",
      "| 0.8852747|\n",
      "|0.17227533|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "time: 1.11 s (started: 2021-06-12 16:03:07 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.select(col('distancia')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando columnas día de la semana, mes, año y hora del viaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### funciones para aplicar a la columna pickupdatetime y obtener las columnas deseadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.25 ms (started: 2021-06-12 16:03:08 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# funciones que me ayudarán en la transformación.\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import calendar\n",
    "def dia(dia):\n",
    "    if dia == 1:\n",
    "        return 'lunes'\n",
    "    if dia == 2:\n",
    "        return 'martes'\n",
    "    if dia == 3:\n",
    "        return 'miércoles'\n",
    "    if dia == 4:\n",
    "        return 'jueves'\n",
    "    if dia == 5:\n",
    "        return 'viernes'\n",
    "    if dia == 6:\n",
    "        return 'sábado'\n",
    "    if dia == 7:\n",
    "        return 'domingo'\n",
    "    if dia < 1 or dia > 7:\n",
    "        return \n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def dia_semana(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    dia_semana = datetime.isoweekday(datetime.strptime(fecha,formato_fecha))\n",
    "    return dia_semana\n",
    "\n",
    "def mes(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    mes = datetime.strptime(fecha,formato_fecha).month\n",
    "    return mes\n",
    "\n",
    "def anio(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    anio = datetime.strptime(fecha,formato_fecha).year\n",
    "    return anio\n",
    "\n",
    "def hora(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_hora = \"%H:%M:%S\"\n",
    "    hora = datetime.strptime(hora,formato_hora).hour\n",
    "    return hora\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# convirtiendo las funciones en funciones UDF\n",
    "udf_dia_semana= udf( lambda z : dia_semana(z), IntegerType())\n",
    "udf_mes= udf( lambda z : mes(z), IntegerType())\n",
    "udf_anio= udf( lambda z : anio(z), IntegerType())\n",
    "udf_hora= udf( lambda z : hora(z), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creando colunma dia de la semana , hora , mes y año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 293 ms (started: 2021-06-12 16:03:08 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfspark_sample = dfspark_sample.withColumn('dia_semana', \n",
    "                                           udf_dia_semana(dfspark_sample['pickup_datetime'] )  )\n",
    "dfspark_sample = dfspark_sample.withColumn('hora', \n",
    "                                           udf_hora(dfspark_sample['pickup_datetime'] )  )\n",
    "\n",
    "dfspark_sample = dfspark_sample.withColumn('mes', \n",
    "                                           udf_mes(dfspark_sample['pickup_datetime'] )  )\n",
    "\n",
    "dfspark_sample = dfspark_sample.withColumn('anio', \n",
    "                                           udf_anio(dfspark_sample['pickup_datetime'] )  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------------+-----------------+------------------+------------------+---------------+--------------------+--------------------+----------+----------+----+---+----+\n",
      "|fare_amount|     pickup_datetime| pickup_longitude|  pickup_latitude| dropoff_longitude|  dropoff_latitude|passenger_count|        dif_latitude|       dif_longitude| distancia|dia_semana|hora|mes|anio|\n",
      "+-----------+--------------------+-----------------+-----------------+------------------+------------------+---------------+--------------------+--------------------+----------+----------+----+---+----+\n",
      "|        8.0|2015-03-12 23:14:...|-73.9931411743164|40.72793960571289|-73.99661254882812|40.744529724121094|              2|0.016590118408203125| 0.00347137451171875| 1.8683056|         4|  23|  3|2015|\n",
      "|       10.0|2013-08-21 08:38:...|       -73.964837|        40.769933|        -73.983462|         40.761655|              1|0.008278000000004226|0.018625000000000114| 1.8191968|         3|   8|  8|2013|\n",
      "|        5.5|2014-01-23 18:40:...|       -74.001017|        40.746352|        -73.990873|         40.739497|              1| 0.00685500000000161|0.010144000000011033|  1.145462|         4|  18|  1|2014|\n",
      "|        6.1|2011-12-24 14:03:...|       -73.982433|        40.768137|        -73.989684|         40.776138|              1|0.008001000000000147|0.007250999999996566| 1.0793539|         6|  14| 12|2011|\n",
      "|        5.0|2012-10-14 23:24:...|       -73.990358|        40.740377|         -74.00085|         40.733955|              1|0.006422000000000594| 0.01049199999999928| 1.1367035|         7|  23| 10|2012|\n",
      "|        5.7|2010-09-27 16:52:...|       -73.990648|        40.763472|        -73.976782|          40.78257|              5|0.019097999999999615| 0.01386599999999305|  2.424119|         1|  16|  9|2010|\n",
      "|       10.2|2009-01-15 22:58:...|       -73.981319|        40.753531|        -73.996766|         40.721148|              2| 0.03238300000000294| 0.01544699999999466|  3.829884|         4|  22|  1|2009|\n",
      "|        4.9|2010-07-11 02:28:...|       -73.991235|        40.727735|        -73.992067|         40.717955|              2|0.009779999999999234|8.320000000026084E-4| 1.0900514|         7|   2|  7|2010|\n",
      "|       22.1|2011-07-24 15:13:...|       -73.948315|        40.724691|        -73.966977|         40.772335|              1| 0.04764399999999824|0.018662000000006174| 5.5276585|         7|  15|  7|2011|\n",
      "|        9.3|2010-11-29 22:04:...|       -73.989308|        40.719017|        -74.006488|         40.743892|              1|0.024875000000001535| 0.01718000000001041|  3.122764|         1|  22| 11|2010|\n",
      "|        6.5|2010-05-23 02:50:...|       -73.978623|         40.78303|        -73.960038|         40.798945|              1|0.015915000000006785|0.018585000000001628| 2.3627946|         7|   2|  5|2010|\n",
      "|       13.0|2012-10-28 01:52:...|       -73.955297|        40.779647|         -73.98095|         40.735482|              3|0.044164999999999566|0.025653000000005477| 5.3667502|         7|   1| 10|2012|\n",
      "|        5.7|2012-02-12 22:13:...|       -73.977564|        40.752028|        -73.980131|         40.763946|              1|0.011917999999994322|0.002566999999999...| 1.3431213|         7|  22|  2|2012|\n",
      "|       16.1|2009-09-10 02:45:...|       -73.985666|        40.767549|        -73.939055|          40.84353|              1| 0.07598099999999874|0.046610999999998626|  9.317738|         4|   2|  9|2009|\n",
      "|       11.7|2012-07-22 02:22:...|       -73.976467|        40.751037|        -73.917138|         40.754415|              1|0.003378000000004988|0.059329000000005294|  5.013023|         7|   2|  7|2012|\n",
      "|       17.3|2011-08-24 19:17:...|           -74.01|         40.72042|        -73.974872|           40.7521|              5|0.031680000000001485| 0.03512800000000027| 4.6022677|         3|  19|  8|2011|\n",
      "|       11.3|2010-01-10 14:36:...|       -73.783215|         40.71171|        -73.789757|          40.70259|              2|0.009119999999995798|0.006541999999996051| 1.1546545|         7|  14|  1|2010|\n",
      "|      57.33|2013-12-05 19:56:...|       -73.788503|        40.647132|        -73.988924|         40.758668|              3| 0.11153600000000097| 0.20042099999999152| 20.964258|         4|  19| 12|2013|\n",
      "|        4.5|2009-06-30 09:59:...|       -73.987928|        40.750743|        -73.988983|         40.758662|              1| 0.00791900000000112|0.001055000000008...| 0.8852747|         2|   9|  6|2009|\n",
      "|        6.5|2015-02-19 11:45:...|-73.9583511352539|40.77812957763672|-73.95774841308594| 40.77960968017578|              5|  0.0014801025390625|  6.0272216796875E-4|0.17227533|         4|  11|  2|2015|\n",
      "+-----------+--------------------+-----------------+-----------------+------------------+------------------+---------------+--------------------+--------------------+----------+----------+----+---+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "time: 604 ms (started: 2021-06-12 16:03:08 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guardando el conjunto de datos  de muestra en un carpeta output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 12s (started: 2021-06-12 16:03:23 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path=\"file:\"+os.getcwd()+\"/Output\"\n",
    "dfspark_sample.write.format(\"csv\").option(\"header\", \"true\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.54 s (started: 2021-06-12 16:07:24 -05:00)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
