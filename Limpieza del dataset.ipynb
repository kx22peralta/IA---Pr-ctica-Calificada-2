{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 388 µs (started: 2021-06-12 15:56:00 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# medir tiempos\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark [Apache spark](https://spark.apache.org/)\n",
    "Apache Spark™ es un motor de análisis unificado para el procesamiento de datos a gran escala.\n",
    "### pyspark\n",
    "PySpark es una interfaz para Apache Spark en Python. No solo le permite escribir aplicaciones Spark utilizando las API de Python, sino que también proporciona el shell de PySpark para analizar interactivamente sus datos en un entorno distribuido. PySpark es compatible con la mayoría de las funciones de Spark, como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) y Spark Core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modulo para encontrar pyspark en el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/spark-3.1.1-bin-hadoop2.7\")    #para linux\n",
    "#findspark.init()                                                 #para windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# Variable de configuración\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"ModeloML\").set(\"spark.driver.maxResultSize\",\"0\")\n",
    "# iniciamos un contexto spark (solo se ejecuta uno. Para ejecutar otra vez , reiniciar el kernel)\n",
    "sc = SparkContext(conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "# le pasamos el contexto anterior\n",
    "sqlContext = SQLContext(sc)\n",
    "dfspark = sqlContext.read.format('csv').option(\"header\",\"true\").option(\"inferSchema\",\"true\").load('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columnas y tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_total=dfspark.count()\n",
    "print(\"total de filas del conjunto de datos\",cant_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tomando una muestra del 4% del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample = dfspark.sample(fraction = 0.04, withReplacement = False, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total de filas de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_muestra=dfspark_sample.count()\n",
    "print(\"total de filas de la muestra del conjunto de datos\",cant_muestra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de la muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procederá a eliminar la columna con la característica \"key\", debido a que contiene datos innecesarios para lograr el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample = dfspark_sample.drop('key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminando Valores Nulos de la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fare_amount (costo de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount is not NULL\")\n",
    "# passenger (número de pasajeros) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"passenger_count is not NULL\")\n",
    "# pickup_datetime (fecha y hora de incio de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_datetime is not NULL\")\n",
    "# pickup (longitud y latitud de inicio de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_latitude is not NULL\")\n",
    "# dropoff (longitud y laitud de fin de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_latitude is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminado valores nan y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabla sin valores nan, sin duplicados\n",
    "dfspark_sample=dfspark_sample.na.drop().dropDuplicates()\n",
    "# cantidad de  data sin nulos ni nan\n",
    "cantnn_muestra=dfspark_sample.count()\n",
    "print(\"Cantidad de filas eliminadas de la muestra: \",cant_muestra-cantnn_muestra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La muestra se guarda en el HDFS \n",
    "Este metodo acelera algunos procesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadisticas de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casteamos a pandas\n",
    "estadisticas=dfspark_sample.describe([\"pickup_longitude\",\n",
    "                                 \"pickup_latitude\",\n",
    "                                 \"dropoff_longitude\",\n",
    "                                 \"dropoff_latitude\",\n",
    "                                 \"passenger_count\",\n",
    "                                 \"fare_amount\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cantidad viajes por número de pasajeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observaciones :\n",
    "1. Valores de passenger_count imposibles , como 34,49,51,129,208.\n",
    "2. Precios demasiados elevados debido a la cantidad de pasajeros y negativo(imposible).\n",
    "3. Cantidad de datos en la que el precio es menor a o igual a 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### minimos y máximos de latitude y longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mínimo y máximo para longitud del subconjunto de datos\n",
    "long_min_i=dfspark_sample.agg({'pickup_longitude': 'min'}).show()\n",
    "long_max_i=dfspark_sample.agg({'pickup_longitude': 'max'}).show()\n",
    "long_min_f=dfspark_sample.agg({'dropoff_longitude': 'min'}).show()\n",
    "long_max_f=dfspark_sample.agg({'dropoff_longitude': 'max'}).show()\n",
    "\n",
    "# mínimo y máximo para para latitud del subconjuntos de datos\n",
    "lat_min_i=dfspark_sample.agg({'pickup_latitude': 'min'}).show()\n",
    "lat_max_i=dfspark_sample.agg({'pickup_latitude': 'max'}).show()\n",
    "lat_min_f=dfspark_sample.agg({'dropoff_latitude': 'min'}).show()\n",
    "lat_max_f=dfspark_sample.agg({'dropoff_latitude': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observaciones:\n",
    "1. Valores para longitud imposibles, ya que longitud varía entre -90 y 90\n",
    "2. Valores para latitud imposibles, ya que latitud varía entre -180 y 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seleccionar passenger_count de 0-9\n",
    "El número de pasajeros de cada viaje puede ser entre 0 a 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample = dfspark_sample.filter(\"passenger_count < 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seleccionar fare_amount mayores o iguales a cero\n",
    "El precio del viaje puede no puede ser menor que cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar fare_amount mayor a 0\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount >= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample.describe([\"fare_amount\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latitud y Longitud validas.\n",
    "La latitud válida varía entre -90 a 90.\n",
    "La longitud válida varía entre -180 a 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando valores grandes para longitud, de tal manera que solo se considerará valores correctos.\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_longitude < 180 and pickup_longitude > -180\" )\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_longitude < 180 and dropoff_longitude > -180\")\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_latitude < 90 and pickup_latitude > -90\" )\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_latitude < 90 and dropoff_latitude > -90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estadisticas=dfspark_sample.describe([\"pickup_longitude\",\n",
    "                                 \"pickup_latitude\",\n",
    "                                 \"dropoff_longitude\",\n",
    "                                 \"dropoff_latitude\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diferencias de latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos columnas de diferencias.\n",
    "from pyspark.sql.functions import abs\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_latitude\",\n",
    "                                           abs(dfspark_sample['dropoff_latitude']-dfspark_sample['pickup_latitude']))\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_longitude\",\n",
    "                                           abs(dfspark_sample['dropoff_longitude']-dfspark_sample['pickup_longitude']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distancia Haversine\n",
    "Es la distancia entre dos puntos geográficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la función para hallar la distancia entre dos puntos geográficos\n",
    "import math\n",
    "from pyspark.sql.functions import udf, array, col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def haversine(x):\n",
    "    lat1=x[0]\n",
    "    lon1=x[1]\n",
    "    lat2=x[2]\n",
    "    lon2=x[3]\n",
    "    \n",
    "    rad=math.pi/180\n",
    "    dlat=lat2-lat1\n",
    "    dlon=lon2-lon1\n",
    "    R=6372.795477598\n",
    "    a=(math.sin(rad*dlat/2))**2 + math.cos(rad*lat1)*math.cos(rad*lat2)*(math.sin(rad*dlon/2))**2\n",
    "    distancia=2*R*math.asin(math.sqrt(a))\n",
    "    return distancia\n",
    "\n",
    "distancia_udf = udf(lambda z: haversine(z), FloatType())\n",
    "#spark.udf.register('distancia_udf', distancia_udf)\n",
    "dfspark_sample = dfspark_sample.withColumn('distancia', distancia_udf(array('pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude')))                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample.select(col('distancia')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando columnas día de la semana, mes, año y hora del viaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### funciones para aplicar a la columna pickupdatetime y obtener las columnas deseadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones que me ayudarán en la transformación.\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import calendar\n",
    "def dia(dia):\n",
    "    if dia == 1:\n",
    "        return 'lunes'\n",
    "    if dia == 2:\n",
    "        return 'martes'\n",
    "    if dia == 3:\n",
    "        return 'miércoles'\n",
    "    if dia == 4:\n",
    "        return 'jueves'\n",
    "    if dia == 5:\n",
    "        return 'viernes'\n",
    "    if dia == 6:\n",
    "        return 'sábado'\n",
    "    if dia == 7:\n",
    "        return 'domingo'\n",
    "    if dia < 1 or dia > 7:\n",
    "        return \n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def dia_semana(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    dia_semana = datetime.isoweekday(datetime.strptime(fecha,formato_fecha))\n",
    "    return dia_semana\n",
    "\n",
    "def mes(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    mes = datetime.strptime(fecha,formato_fecha).month\n",
    "    return mes\n",
    "\n",
    "def anio(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    anio = datetime.strptime(fecha,formato_fecha).year\n",
    "    return anio\n",
    "\n",
    "def hora(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_hora = \"%H:%M:%S\"\n",
    "    hora = datetime.strptime(hora,formato_hora).hour\n",
    "    return hora\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# convirtiendo las funciones en funciones UDF\n",
    "udf_dia_semana= udf( lambda z : dia_semana(z), IntegerType())\n",
    "udf_mes= udf( lambda z : mes(z), IntegerType())\n",
    "udf_anio= udf( lambda z : anio(z), IntegerType())\n",
    "udf_hora= udf( lambda z : hora(z), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creando colunma dia de la semana , hora , mes y año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfspark_sample = dfspark_sample.withColumn('dia_semana', \n",
    "                                           udf_dia_semana(dfspark_sample['pickup_datetime'] )  )\n",
    "dfspark_sample = dfspark_sample.withColumn('hora', \n",
    "                                           udf_hora(dfspark_sample['pickup_datetime'] )  )\n",
    "\n",
    "dfspark_sample = dfspark_sample.withColumn('mes', \n",
    "                                           udf_mes(dfspark_sample['pickup_datetime'] )  )\n",
    "\n",
    "dfspark_sample = dfspark_sample.withColumn('anio', \n",
    "                                           udf_anio(dfspark_sample['pickup_datetime'] )  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Guardando el conjunto de datos  de muestra en un carpeta output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=\"file:\"+os.getcwd()+\"/Output\"\n",
    "dfspark_sample.write.format(\"csv\").option(\"header\", \"true\").save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
