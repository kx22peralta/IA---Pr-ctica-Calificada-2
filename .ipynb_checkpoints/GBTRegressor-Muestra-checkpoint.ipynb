{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medir tiempos\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/spark-3.1.1-bin-hadoop2.7\")    #para linux\n",
    "#findspark.init()                                                 #para windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# Variable de configuraci√≥n\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"GBTRegressor\").set(\"spark.driver.maxResultSize\",\"0\")\n",
    "# iniciamos un contexto spark (solo se ejecuta uno. Para ejecutar otra vez , reiniciar el kernel)\n",
    "sc = SparkContext(conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "# le pasamos el contexto anterior\n",
    "sqlContext = SQLContext(sc)\n",
    "import os\n",
    "path=\"file:\"+os.getcwd()+\"/Output\"\n",
    "dfspark = sqlContext.read.format('csv').option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(path+'/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### creando un vector de caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "ignore=['fare_amount',\n",
    "        'pickup_datetime',\n",
    "        'pickup_longitude',\n",
    "       'pickup_latitude',\n",
    "       'dropoff_longitude',\n",
    "       'dropoff_latitude',\n",
    "       'dif_latitude',\n",
    "       'dif_longitude']\n",
    "vectorAssembler = VectorAssembler(inputCols=[x for x in dfspark.columns  \n",
    "                  if x not in ignore], outputCol = 'features')\n",
    "df = vectorAssembler.transform(dfspark)\n",
    "dfmodel = df.select(['features', 'fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar la data en 80% entrenamiento y 20% test\n",
    "dftrain, dftest = dfmodel.randomSplit([0.8, 0.2])\n",
    "print(\"tenemos %d datos de entrenamiento y %d datos de prueba.\" % (dftrain.count(), dftest.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"fare_amount\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtrparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 5, 8])\n",
    "             .addGrid(gbt.maxBins, [10, 20, 40])\n",
    "             .addGrid(gbt.maxIter, [5, 10, 20])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"fare_amount\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3-fold CrossValidator\n",
    "gbcv = CrossValidator(estimator = gbt,\n",
    "                      estimatorParamMaps = gbtrparamGrid,\n",
    "                      evaluator = gbevaluator,\n",
    "                      numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcvModel = gbcv.fit(dftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbcvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbpredictions = gbcvModel.transform(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE:', gbevaluator.evaluate(gbpredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"file:\"+os.getcwd()+\"/models/\"\n",
    "gbt.save(path+\"gbtregressor\")\n",
    "gbcv.save(path+\"gbcrosvalidator\")\n",
    "gbcvModel.save(path+\"Model\")\n",
    "gbevaluator.save(path+\"evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
