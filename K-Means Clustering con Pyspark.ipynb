{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08f4332",
   "metadata": {},
   "source": [
    "### Aprenidzaje No supervizado (Solo variables independientes)\n",
    "es un tipo de apÂ´rendizaje no supervizado, que se utiliza cuando tiiee datos no etiquetados es decir datos sin categorias o grupos definidos es decir datos sin categoria o grupos definidos\n",
    "El objetivo de ewste algoritmo es encontrar grupos en los datos.\n",
    "los puntos de datos se agrupan segun la similitud de caracteristicas \n",
    "las utilidades de este tipo de aprendizaje se dan por ejemplo:\n",
    "celulas cancerosa, buscar pallabras , identificar valores atipicos en el comportamiento\n",
    "\n",
    "En este caso se esta aplicando a la data de en la cuales  tiene como variables\n",
    "\n",
    "en este caso tomamos solo los valores independientes como son la latitud , longitud fecha , hora ,numero depasajeros, dia semana \n",
    "\n",
    "###### solo variables independientes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1e218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "#findspark.init(\"/usr/local/spark/spark-3.1.1-bin-hadoop2.7\")    #para linux\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae49d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------------+-----------------+------------------+------------------+---------------+--------------------+--------------------+---------+----------+----+\n",
      "|fare_amount|     pickup_datetime| pickup_longitude|  pickup_latitude| dropoff_longitude|  dropoff_latitude|passenger_count|        dif_latitude|       dif_longitude|distancia|dia_semana|hora|\n",
      "+-----------+--------------------+-----------------+-----------------+------------------+------------------+---------------+--------------------+--------------------+---------+----------+----+\n",
      "|        8.0|2015-03-12 23:14:...|-73.9931411743164|40.72793960571289|-73.99661254882812|40.744529724121094|              2|0.016590118408203125| 0.00347137451171875|1.8683056|         4|  23|\n",
      "|       10.0|2013-08-21 08:38:...|       -73.964837|        40.769933|        -73.983462|         40.761655|              1|0.008278000000004226|0.018625000000000114|1.8191968|         3|   8|\n",
      "|        5.5|2014-01-23 18:40:...|       -74.001017|        40.746352|        -73.990873|         40.739497|              1| 0.00685500000000161|0.010144000000011033| 1.145462|         4|  18|\n",
      "|        6.1|2011-12-24 14:03:...|       -73.982433|        40.768137|        -73.989684|         40.776138|              1|0.008001000000000147|0.007250999999996566|1.0793539|         6|  14|\n",
      "|        5.0|2012-10-14 23:24:...|       -73.990358|        40.740377|         -74.00085|         40.733955|              1|0.006422000000000594| 0.01049199999999928|1.1367035|         7|  23|\n",
      "+-----------+--------------------+-----------------+-----------------+------------------+------------------+---------------+--------------------+--------------------+---------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# instantiate spark environment\n",
    "sc = SparkContext(\"local[*]\", \"Decision Tree Classifier\")\n",
    "spark = SparkSession(sc)\n",
    "# load the dataset\n",
    "df = spark.read.csv(\"newDatapandas.csv\", inferSchema=True, header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc2a674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fare_amount',\n",
       " 'pickup_datetime',\n",
       " 'pickup_longitude',\n",
       " 'pickup_latitude',\n",
       " 'dropoff_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'passenger_count',\n",
       " 'dif_latitude',\n",
       " 'dif_longitude',\n",
       " 'distancia',\n",
       " 'dia_semana',\n",
       " 'hora']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf069260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "columns = ['pickup_longitude',\n",
    " 'pickup_latitude',\n",
    " 'dropoff_longitude',\n",
    " 'dropoff_latitude',\n",
    " 'passenger_count',\n",
    " 'dia_semana',\n",
    " 'hora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eee2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lo que hacemos es tomar un conjunto de columnas y definir una caracteristuca en particular pra nuestras caracteristuicas\n",
    "\n",
    "vect_assembler = VectorAssembler(inputCols = columns, outputCol ='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a623cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = vect_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1aea9",
   "metadata": {},
   "source": [
    "la estandarizacion del metodo de tranformacion del conjunto de datos es unm requisito comun para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe43c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af94fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol =\"features\", outputCol =\"scaledFeatures\", withMean=False, withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e49cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerModel = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a70ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e764a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans3 = KMeans(featuresCol ='scaledFeatures', k =3)\n",
    "kmeans2 = KMeans(featuresCol ='scaledFeatures', k =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f401f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k3 =kmeans3.fit(cluster_final_data)\n",
    "model_k2 =kmeans2.fit(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea5cf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = model_k3.transform(cluster_final_data)\n",
    "predictions2 = model_k2.transform(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afba74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15f12a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_3 = evaluator.evaluate(predictions3)\n",
    "silhouette_2 = evaluator.evaluate(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbc02b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con k =3\n",
      "Suuma de los errores al cuadrado = -0.014664806474756986\n",
      "Con k =2\n",
      "Suuma de los errores al cuadrado = 0.9593956831344325\n"
     ]
    }
   ],
   "source": [
    "print(\"Con k =3\")\n",
    "print(\"Suuma de los errores al cuadrado = \" + str(silhouette_3))\n",
    "print(\"Con k =2\")\n",
    "print(\"Suuma de los errores al cuadrado = \" + str(silhouette_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe390589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con k =2\n",
      "Suuma de los errores al cuadrado = 0.9593956831344325\n",
      "Con k =3\n",
      "Suuma de los errores al cuadrado = -0.014664806474756986\n",
      "Con k =4\n",
      "Suuma de los errores al cuadrado = -0.5592767797892542\n",
      "Con k =5\n",
      "Suuma de los errores al cuadrado = -0.016216138812811413\n",
      "Con k =6\n",
      "Suuma de los errores al cuadrado = -0.06916233321257086\n",
      "Con k =7\n",
      "Suuma de los errores al cuadrado = -0.07510872544253545\n",
      "Con k =8\n",
      "Suuma de los errores al cuadrado = -0.16805817365567766\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,9):\n",
    "    kmeans  = KMeans(featuresCol ='scaledFeatures', k =k)\n",
    "    model =kmeans.fit(cluster_final_data)\n",
    "    predictions = model.transform(cluster_final_data)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print(\"Con k ={}\".format(k))\n",
    "    print(\"Suuma de los errores al cuadrado = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22da7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|prediction|  count|\n",
      "+----------+-------+\n",
      "|         1|  45295|\n",
      "|         2|1074777|\n",
      "|         0|1099101|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k3.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6023a6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|prediction|  count|\n",
      "+----------+-------+\n",
      "|         1|  45295|\n",
      "|         0|2173878|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k2.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d02faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84ccea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ed31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d3a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be906e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd942c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f650a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59732625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd894b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
