{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de tarifas de taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es construir un modelo de aprendizaje que sea capaz de predecir la tarifa que cobra un taxi de acuerdo a un conjunto de datos de entrada, el cual está compuesto por un archivo CSV que contiene alrededor de 55 millones de registros de viajes en taxi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada registro contiene la siguiente información:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ID: cadena que identifica de manera única a cada registro\n",
    "2. pickup_datetime: timestamp indicando cuando el viaje a empezado\n",
    "3. pickup_longitude: número real indicando la ubicación en longitud en donde el viaje empezó\n",
    "4. pickup_latitude: número real indicando la ubicación en latitud en donde el viaje empezó\n",
    "5. dropoff_longitude: número real indicando la ubicación en longitud en donde el viaje terminó\n",
    "6. dropoff_latitude: número real indicando la ubicación en latitud en donde el viaje terminó\n",
    "7. passenger_count: número entero indicando el número de pasajeros en el servicio de taxi\n",
    "8. fare_amount: número real indicando el costo del taxi. Esta es la variable a predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando la función para medir el tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-06-13 17:34:02 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# medir tiempos\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente imagen se puede visualizar que para cargar el archivo CSV que contiene toda la data, tarda aproximadamente 37 min, lo que implica una desventaja a la hora de entrenar un modelo. Por eso se opta por trabajar en paralelo, usando PySpark, que acelerará el proceso de la carga de todo el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output](tiempo_secuencial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando el conjunto de datos usando Pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-06-13 17:34:03 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Modulo para encontrar pyspark\n",
    "import findspark\n",
    "# findspark.init(\"/usr/local/spark/spark-3.1.1-bin-hadoop2.7\")    #para linux\n",
    "findspark.init()                                                 #para windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.19:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ModeloML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=ModeloML>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.58 s (started: 2021-06-13 17:34:04 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Importamos pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# Variable de configuración\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"ModeloML\")\n",
    "# Iniciamos un contexto spark (solo se ejecuta uno. Para ejecutar otra vez , reiniciar el kernel)\n",
    "sc = SparkContext(conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.4 s (started: 2021-06-13 17:34:10 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "# le pasamos el contexto anterior\n",
    "sqlContext = SQLContext(sc)\n",
    "dfspark = sqlContext.read.format('csv').option(\"header\",\"true\").option(\"inferSchema\",\"true\").load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n",
      "time: 16 ms (started: 2021-06-13 17:34:47 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Esquema de los datos\n",
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55423856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.34 s (started: 2021-06-13 17:34:47 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Total de cantidad de datos\n",
    "cant_total=dfspark.count()\n",
    "cant_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-06-13 17:34:54 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Tomando una muestra del 4% del total\n",
    "dfspark_sample = dfspark.sample(fraction = 0.04, withReplacement = False, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apartir de aqui trabajamos con una muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2219408"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.36 s (started: 2021-06-13 17:34:59 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de datos para la muestra\n",
    "cant_muestra=dfspark_sample.count()\n",
    "cant_muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procederá a eliminar la columna con la característica \"key\", debido a que contiene datos innecesarios para lograr el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-06-13 17:36:36 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample = dfspark_sample.drop('key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminando Valores Nulos de la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 313 ms (started: 2021-06-13 17:36:38 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# fare_amount (costo de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount is not NULL\")\n",
    "# passenger (número de pasajeros) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"passenger_count is not NULL\")\n",
    "# pickup_datetime (fecha y hora de incio de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_datetime is not NULL\")\n",
    "# pickup (longitud y latitud de inicio de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_latitude is not NULL\")\n",
    "# dropoff (longitud y laitud de fin de viaje) no nulos\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_latitude is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminado valores nan y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms (started: 2021-06-13 17:36:39 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Tabla sin valores nan, sin duplicados\n",
    "dfspark_sample=dfspark_sample.na.drop().dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fare_amount: double, pickup_datetime: string, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, passenger_count: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2021-06-13 17:36:39 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Que el comjunto de datos persista en memoria, acelera algunos procesos.\n",
    "dfspark_sample.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas eliminadas de la muestra:  18\n",
      "Nueva cantidad de datos:  2219390\n",
      "time: 35 s (started: 2021-06-13 17:36:42 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de datos sin nulos, ni nan, ni duplicados\n",
    "nueva_cant_muestra=dfspark_sample.count()\n",
    "print(\"Cantidad de filas eliminadas de la muestra: \",cant_muestra-nueva_cant_muestra)\n",
    "print(\"Nueva cantidad de datos: \",nueva_cant_muestra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 594 ms (started: 2021-06-13 17:37:17 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.11 s (started: 2021-06-13 17:37:18 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Casteamos a pandas\n",
    "summary=dfspark_sample.describe([\"pickup_longitude\",\n",
    "                                 \"pickup_latitude\",\n",
    "                                 \"dropoff_longitude\",\n",
    "                                 \"dropoff_latitude\",\n",
    "                                 \"passenger_count\",\n",
    "                                 \"fare_amount\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "      <td>2219390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>-72.50355231460159</td>\n",
       "      <td>39.91779369363991</td>\n",
       "      <td>-72.50697030839864</td>\n",
       "      <td>39.91850726486102</td>\n",
       "      <td>1.6851959322156087</td>\n",
       "      <td>11.352503967306385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>13.098889190132796</td>\n",
       "      <td>9.821243657749829</td>\n",
       "      <td>13.244549098861283</td>\n",
       "      <td>10.317917931047642</td>\n",
       "      <td>1.3377753769552685</td>\n",
       "      <td>42.449025079080954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-3366.527908</td>\n",
       "      <td>-3488.079513</td>\n",
       "      <td>-3366.527908</td>\n",
       "      <td>-3488.079513</td>\n",
       "      <td>0</td>\n",
       "      <td>-52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>2497.117435</td>\n",
       "      <td>2964.163855</td>\n",
       "      <td>3211.57975</td>\n",
       "      <td>3333.304575</td>\n",
       "      <td>208</td>\n",
       "      <td>61550.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary    pickup_longitude    pickup_latitude   dropoff_longitude  \\\n",
       "0   count             2219390            2219390             2219390   \n",
       "1    mean  -72.50355231460159  39.91779369363991  -72.50697030839864   \n",
       "2  stddev  13.098889190132796  9.821243657749829  13.244549098861283   \n",
       "3     min        -3366.527908       -3488.079513        -3366.527908   \n",
       "4     max         2497.117435        2964.163855          3211.57975   \n",
       "\n",
       "     dropoff_latitude     passenger_count         fare_amount  \n",
       "0             2219390             2219390             2219390  \n",
       "1   39.91850726486102  1.6851959322156087  11.352503967306385  \n",
       "2  10.317917931047642  1.3377753769552685  42.449025079080954  \n",
       "3        -3488.079513                   0               -52.0  \n",
       "4         3333.304575                 208            61550.86  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-06-13 17:37:51 -05:00)\n"
     ]
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|passenger_count|  count|\n",
      "+---------------+-------+\n",
      "|              1|1535882|\n",
      "|              6|  47132|\n",
      "|              3|  97242|\n",
      "|              5| 157254|\n",
      "|              4|  47158|\n",
      "|              7|      1|\n",
      "|              2| 326983|\n",
      "|              0|   7734|\n",
      "|            208|      4|\n",
      "+---------------+-------+\n",
      "\n",
      "time: 2.66 s (started: 2021-06-13 17:37:54 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Agrupación por cantidad de pasajeros.\n",
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones :\n",
    "1. Valores de passenger_count imposibles , como 34,49,51,129,208.\n",
    "2. Precios demasiados elevados debido a la cantidad de pasajeros y negativo(imposible).\n",
    "3. Cantidad de datos en la que el precio es menor a o igual a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|min(pickup_longitude)|\n",
      "+---------------------+\n",
      "|         -3366.527908|\n",
      "+---------------------+\n",
      "\n",
      "+---------------------+\n",
      "|max(pickup_longitude)|\n",
      "+---------------------+\n",
      "|          2497.117435|\n",
      "+---------------------+\n",
      "\n",
      "+----------------------+\n",
      "|min(dropoff_longitude)|\n",
      "+----------------------+\n",
      "|          -3366.527908|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|max(dropoff_longitude)|\n",
      "+----------------------+\n",
      "|            3211.57975|\n",
      "+----------------------+\n",
      "\n",
      "+--------------------+\n",
      "|min(pickup_latitude)|\n",
      "+--------------------+\n",
      "|        -3488.079513|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|max(pickup_latitude)|\n",
      "+--------------------+\n",
      "|         2964.163855|\n",
      "+--------------------+\n",
      "\n",
      "+---------------------+\n",
      "|min(dropoff_latitude)|\n",
      "+---------------------+\n",
      "|         -3488.079513|\n",
      "+---------------------+\n",
      "\n",
      "+---------------------+\n",
      "|max(dropoff_latitude)|\n",
      "+---------------------+\n",
      "|          3333.304575|\n",
      "+---------------------+\n",
      "\n",
      "time: 5.45 s (started: 2021-06-13 17:37:56 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Mínimo y máximo para longitud del subconjunto de datos\n",
    "long_min_i=dfspark_sample.agg({'pickup_longitude': 'min'}).show()\n",
    "long_max_i=dfspark_sample.agg({'pickup_longitude': 'max'}).show()\n",
    "long_min_f=dfspark_sample.agg({'dropoff_longitude': 'min'}).show()\n",
    "long_max_f=dfspark_sample.agg({'dropoff_longitude': 'max'}).show()\n",
    "\n",
    "# Mínimo y máximo para para latitud del subconjuntos de datos\n",
    "lat_min_i=dfspark_sample.agg({'pickup_latitude': 'min'}).show()\n",
    "lat_max_i=dfspark_sample.agg({'pickup_latitude': 'max'}).show()\n",
    "lat_min_f=dfspark_sample.agg({'dropoff_latitude': 'min'}).show()\n",
    "lat_max_f=dfspark_sample.agg({'dropoff_latitude': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones:\n",
    "1. Valores para longitud imposibles, ya que longitud varía entre -90 y 90\n",
    "2. Valores para latitud imposibles, ya que latitud varía entre -180 y 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformación de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46 ms (started: 2021-06-13 17:38:05 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar passenger_count de 0-9\n",
    "dfspark_sample = dfspark_sample.filter(\"passenger_count < 10\")\n",
    "# Selecionar fare_amount mayor a 0\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount >= 0 and fare_amount <= 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|passenger_count|  count|\n",
      "+---------------+-------+\n",
      "|              1|1535151|\n",
      "|              6|  47119|\n",
      "|              3|  97212|\n",
      "|              5| 157231|\n",
      "|              4|  47132|\n",
      "|              7|      1|\n",
      "|              2| 326869|\n",
      "|              0|   7734|\n",
      "+---------------+-------+\n",
      "\n",
      "time: 2.74 s (started: 2021-06-13 17:38:05 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2021-06-13 17:38:08 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Filtrando valores grandes para longitud, de tal manera que solo se considerará valores correctos.\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_longitude < 180 and pickup_longitude > -180\" )\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_longitude < 180 and dropoff_longitude > -180\")\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_latitude < 90 and pickup_latitude > -90\" )\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_latitude < 90 and dropoff_latitude > -90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.75 s (started: 2021-06-13 17:38:08 -05:00)\n"
     ]
    }
   ],
   "source": [
    "estadisticas=dfspark_sample.describe([\"pickup_longitude\",\n",
    "                                 \"pickup_latitude\",\n",
    "                                 \"dropoff_longitude\",\n",
    "                                 \"dropoff_latitude\",\n",
    "                                  \"fare_amount\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>2218324</td>\n",
       "      <td>2218324</td>\n",
       "      <td>2218324</td>\n",
       "      <td>2218324</td>\n",
       "      <td>2218324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>-72.49792642807238</td>\n",
       "      <td>39.9178324326265</td>\n",
       "      <td>-72.50456776388887</td>\n",
       "      <td>39.92090591335847</td>\n",
       "      <td>11.277273396492163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>10.456633559991248</td>\n",
       "      <td>6.108952929695163</td>\n",
       "      <td>10.431325659855101</td>\n",
       "      <td>6.10028741263642</td>\n",
       "      <td>9.39446820001931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-121.91499328613281</td>\n",
       "      <td>-74.017222</td>\n",
       "      <td>-121.9151840209961</td>\n",
       "      <td>-74.177303</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>40.840962</td>\n",
       "      <td>74.007413</td>\n",
       "      <td>73.93996</td>\n",
       "      <td>74.95</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary     pickup_longitude    pickup_latitude   dropoff_longitude  \\\n",
       "0   count              2218324            2218324             2218324   \n",
       "1    mean   -72.49792642807238   39.9178324326265  -72.50456776388887   \n",
       "2  stddev   10.456633559991248  6.108952929695163  10.431325659855101   \n",
       "3     min  -121.91499328613281         -74.017222  -121.9151840209961   \n",
       "4     max            40.840962          74.007413            73.93996   \n",
       "\n",
       "    dropoff_latitude         fare_amount  \n",
       "0            2218324             2218324  \n",
       "1  39.92090591335847  11.277273396492163  \n",
       "2   6.10028741263642    9.39446820001931  \n",
       "3         -74.177303                 0.0  \n",
       "4              74.95               100.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-06-13 17:38:10 -05:00)\n"
     ]
    }
   ],
   "source": [
    "estadisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2021-06-13 17:38:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Agregamos columnas de diferencias.\n",
    "from pyspark.sql.functions import abs\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_latitude\",\n",
    "                                           abs(dfspark_sample['dropoff_latitude']-dfspark_sample['pickup_latitude']))\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_longitude\",\n",
    "                                           abs(dfspark_sample['dropoff_longitude']-dfspark_sample['pickup_longitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 62 ms (started: 2021-06-13 17:38:14 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Creamos la función para hallar la distancia entre dos puntos geográficos\n",
    "import math\n",
    "from pyspark.sql.functions import udf, array, col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def haversine(x):\n",
    "    lat1=x[0]\n",
    "    lon1=x[1]\n",
    "    lat2=x[2]\n",
    "    lon2=x[3]\n",
    "    \n",
    "    rad=math.pi/180\n",
    "    dlat=lat2-lat1\n",
    "    dlon=lon2-lon1\n",
    "    R=6372.795477598\n",
    "    a=(math.sin(rad*dlat/2))**2 + math.cos(rad*lat1)*math.cos(rad*lat2)*(math.sin(rad*dlon/2))**2\n",
    "    distancia=2*R*math.asin(math.sqrt(a))\n",
    "    return distancia\n",
    "\n",
    "distancia_udf = udf(lambda z: haversine(z), FloatType())\n",
    "#spark.udf.register('distancia_udf', distancia_udf)\n",
    "dfspark_sample = dfspark_sample.withColumn('distancia', distancia_udf(array('pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude')))                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| distancia|\n",
      "+----------+\n",
      "| 1.8683056|\n",
      "| 1.8191968|\n",
      "|  1.145462|\n",
      "| 1.0793539|\n",
      "| 1.1367035|\n",
      "|  2.424119|\n",
      "|  3.829884|\n",
      "| 1.0900514|\n",
      "| 5.5276585|\n",
      "|  3.122764|\n",
      "| 2.3627946|\n",
      "| 5.3667502|\n",
      "| 1.3431213|\n",
      "|  9.317738|\n",
      "|  5.013023|\n",
      "| 4.6022677|\n",
      "| 1.1546545|\n",
      "| 20.964258|\n",
      "| 0.8852747|\n",
      "|0.17227533|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "time: 1.24 s (started: 2021-06-13 17:38:16 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.select(col('distancia')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-06-13 17:38:20 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Creamos dos columnas día de la semana y hora del viaje.\n",
    "# funciones que me ayudarán en la transformación.\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import calendar\n",
    "def dia(dia):\n",
    "    if dia == 1:\n",
    "        return 'lunes'\n",
    "    if dia == 2:\n",
    "        return 'martes'\n",
    "    if dia == 3:\n",
    "        return 'miércoles'\n",
    "    if dia == 4:\n",
    "        return 'jueves'\n",
    "    if dia == 5:\n",
    "        return 'viernes'\n",
    "    if dia == 6:\n",
    "        return 'sábado'\n",
    "    if dia == 7:\n",
    "        return 'domingo'\n",
    "    if dia < 1 or dia > 7:\n",
    "        return \n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def dia_semana(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    dia_semana = datetime.isoweekday(datetime.strptime(fecha,formato_fecha))\n",
    "    return dia_semana\n",
    "\n",
    "def mes(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    mes = datetime.strptime(fecha,formato_fecha).month\n",
    "    return mes\n",
    "\n",
    "def anio(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    anio = datetime.strptime(fecha,formato_fecha).year\n",
    "    return anio\n",
    "\n",
    "def hora(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_hora = \"%H:%M:%S\"\n",
    "    hora = datetime.strptime(hora,formato_hora).hour\n",
    "    return hora\n",
    "    \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Convirtiendo las funciones en funciones UDF\n",
    "udf_dia_semana= udf( lambda z : dia_semana(z), IntegerType())\n",
    "udf_mes= udf( lambda z : mes(z), IntegerType())\n",
    "udf_anio= udf( lambda z : anio(z), IntegerType())\n",
    "udf_hora= udf( lambda z : hora(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78 ms (started: 2021-06-13 17:38:22 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfspark_sample = dfspark_sample.withColumn('dia_semana', \n",
    "                                           udf_dia_semana(dfspark_sample['pickup_datetime'] )  )\n",
    "dfspark_sample = dfspark_sample.withColumn('hora', \n",
    "                                           udf_hora(dfspark_sample['pickup_datetime'] )  )\n",
    "dfspark_sample = dfspark_sample.withColumn('mes', \n",
    "                                           udf_mes(dfspark_sample['pickup_datetime'] )  )\n",
    "dfspark_sample = dfspark_sample.withColumn('anio', \n",
    "                                           udf_anio(dfspark_sample['pickup_datetime'] )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2021-06-13 17:38:22 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Castear dia de la semana y hora\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dia_semana\",\n",
    "                                           dfspark_sample[\"dia_semana\"].cast(\"Integer\"))\n",
    "dfspark_sample = dfspark_sample.withColumn(\"hora\",\n",
    "                                           dfspark_sample[\"hora\"].cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- dif_latitude: double (nullable = true)\n",
      " |-- dif_longitude: double (nullable = true)\n",
      " |-- distancia: float (nullable = true)\n",
      " |-- dia_semana: integer (nullable = true)\n",
      " |-- hora: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      "\n",
      "time: 0 ns (started: 2021-06-13 17:38:23 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guardando el conjunto de datos  en un carpeta output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 32s (started: 2021-06-13 17:41:54 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "## path=\"file:\"+os.getcwd()+\"/Output\"                  ## para linux\n",
    "path=os.getcwd()+\"/Output\"                             ## para windows   \n",
    "dfspark_sample.write.format(\"csv\").option(\"header\", \"true\").save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadística de las nuevas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_summary=dfspark_sample.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fc8cba4ebbfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;34m\"hora\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \"distancia\"]\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mclean_summary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumnas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_summary' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-06-13 17:37:21 -05:00)\n"
     ]
    }
   ],
   "source": [
    "columnas = [\"summary\",\n",
    "            \"pickup_longitude\",\n",
    "            \"pickup_latitude\",\n",
    "            \"dropoff_longitude\",\n",
    "            \"dropoff_latitude\",\n",
    "            \"passenger_count\",\n",
    "            \"fare_amount\",\n",
    "            \"dif_latitude\",\n",
    "            \"dif_longitude\",\n",
    "            \"dia_semana\",\n",
    "            \"mes\",\n",
    "            \"anio\",\n",
    "            \"hora\",\n",
    "            \"distancia\"]\n",
    "clean_summary[columnas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlación de los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_old=[\"pickup_longitude\",\n",
    "         \"pickup_latitude\",\n",
    "         \"dropoff_longitude\",\n",
    "         \"dropoff_latitude\",\n",
    "         \"passenger_count\"]\n",
    "col_new=[\"dif_latitude\",\n",
    "         \"dif_longitude\",\n",
    "         \"dia_semana\",\n",
    "         \"mes\",\n",
    "         \"anio\"\n",
    "         \"hora\",\n",
    "         \"distancia\"]\n",
    "col_pred = [\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación en un vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "inputCols = col_old+col_new+col_pred\n",
    "assembler = VectorAssembler( inputCols=inputCols, outputCol=\"col_corr\")\n",
    "dfspark_corr = assembler.transform(dfspark_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_corr=dfspark_corr.select('col_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estudiar la correlación\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "dfspark_corr= Correlation.corr(dfspark_corr, 'col_corr','pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "#type(dfspark_cor.collect()[0][0]) # denseMatrix\n",
    "# Pasamos la matrix como un array.\n",
    "array_corr=dfspark_corr.collect()[0][0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corr= pd.DataFrame(array_corr, columns=inputCols, index=inputCols)\n",
    "mask = ~(pdf_corr>-0.3) | ~(pdf_corr<0.3)\n",
    "round(pdf_corr,10).style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observación:\n",
    "###### La variable a predecir fare_amount tiene muy baja correlación con las demás variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de la data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos toda la data a Pandas\n",
    "pandasData = dfspark_sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manera alternativa de pasar la data a Pandas\n",
    "# import pandas as pd\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# # Wrapper for seamless Spark's serialisation\n",
    "# def spark_to_pandas(spark_df: DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     PySpark toPandas realisation using mapPartitions\n",
    "#     much faster than vanilla version\n",
    "#     fork: https://gist.github.com/lucidyan/1e5d9e490a101cdc1c2ed901568e082b\n",
    "#     origin: https://gist.github.com/joshlk/871d58e01417478176e7\n",
    "#     :param spark_df:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def _map_to_pandas(rdds) -> list:\n",
    "#         \"\"\" Needs to be here due to pickling issues \"\"\"\n",
    "#         return [pd.DataFrame(list(rdds))]\n",
    "\n",
    "#     def _to_pandas(df: DataFrame, n_partitions: int = None) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Returns the contents of `df` as a local `pandas.DataFrame` in a speedy fashion. The DataFrame is\n",
    "#         repartitioned if `n_partitions` is passed.\n",
    "#         :param df:\n",
    "#         :param n_partitions:\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         if n_partitions is not None:\n",
    "#             df = df.repartition(n_partitions)\n",
    "#         df_pand = df.rdd.mapPartitions(_map_to_pandas).collect()  # type: pd.DataFrame\n",
    "#         df_pand = pd.concat(df_pand)\n",
    "#         df_pand.columns = df.columns\n",
    "#         return df_pand\n",
    "\n",
    "#     return _to_pandas(spark_df)\n",
    "\n",
    "# pandasData = spark_to_pandas(dfspark_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostrando la data\n",
    "display(pandasData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\" , color_codes = True)\n",
    "var = [\"dia_semana\"]\n",
    "# 1 inches = 96px\n",
    "g = sns.pairplot(pandasData[:10], vars=var, diag_kind=\"hist\", hue='dia_semana', height=4, aspect=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuanto de la variable objetivo va variando segun los dias de la semana\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "carac1=pandasData['dia_semana']\n",
    "objet=pandasData['fare_amount']\n",
    "plt.plot(carac1, objet, 'o')\n",
    "plt.xlabel(\"Característica dia de la semana\")\n",
    "plt.ylabel(\"Objetivo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#como los pasajeros se distribuyen a traves de las horas\n",
    "pandasData.groupby('hora')['passenger_count'].sum().plot(kind='barh',legend='Reverse',figsize=(10,10))\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#como se reparten los pasajeros en funcion de la hora\n",
    "pandasData.passenger_count.groupby(pandasData.hora).sum().plot(kind='pie',cmap='Paired',figsize=(12,8))\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dispersion de las características\n",
    "caracteristicas=pandasData[['fare_amount','pickup_longitude','pickup_latitude','dropoff_longitude']]\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "g= sns.pairplot(caracteristicas,hue='fare_amount',palette='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.lmplot(x='passenger_count',y='hora',data=caracteristicas,palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relación entre dos caracteristicas vista como tendencia lineal\n",
    "g=sns.lmplot(x='passenger_count',y='hora',hue='dia_semana',data=caracteristicas,palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cerramos la sesión spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
