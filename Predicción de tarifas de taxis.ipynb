{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 ms (started: 2021-05-24 19:07:33 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# medir tiempos\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output](tiempo_secuencial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paralelo pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 148 ms (started: 2021-05-24 19:07:34 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Modulo para encontrar pyspark\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/spark-3.1.1-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ModeloML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=ModeloML>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.7 s (started: 2021-05-24 19:07:35 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# importamos pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# Variable de configuración\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ModeloML\")\n",
    "# iniciamos un contexto spark (solo se ejecuta uno. Para ejecutar otra vez , reiniciar el kernel)\n",
    "sc = SparkContext(conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 49s (started: 2021-05-24 19:08:55 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "# le pasamos el contexto anterior\n",
    "sqlContext = SQLContext(sc)\n",
    "dfspark = sqlContext.read.format('csv').option(\"header\",\"true\").option(\"inferSchema\",\"true\").load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n",
      "time: 10.1 ms (started: 2021-05-24 19:11:45 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Esquema de los datos\n",
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.7 s (started: 2021-05-24 19:11:45 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#cantidad de datos\n",
    "cant_total=dfspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.45 ms (started: 2021-05-24 19:12:11 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Tomar una muestra\n",
    "dfspark_sample = dfspark.sample(fraction = 0.04, withReplacement = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartir de aqui trabajamos con una muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.5 s (started: 2021-05-24 19:12:11 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# cantidad de muestra\n",
    "cant_muestra=dfspark_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminando Valores Nulos de la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 244 ms (started: 2021-05-24 19:12:36 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#fare_amount no nulo\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount is not NULL\")\n",
    "# pasajeros no nulo\n",
    "dfspark_sample = dfspark_sample.filter(\"passenger_count is not NULL\")\n",
    "#pickup_datetime\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_datetime is not NULL\")\n",
    "#pickup\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"pickup_latitude is not NULL\")\n",
    "# dropoff\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_longitude is not NULL\")\n",
    "dfspark_sample = dfspark_sample.filter(\"dropoff_latitude is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminado valores nan y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de valores nulos:  11\n",
      "time: 2min 34s (started: 2021-05-24 19:12:36 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# tabla sin valores nan, sin duplicados\n",
    "dfspark_sample=dfspark_sample.na.drop().dropDuplicates()\n",
    "# cantidad de  data sin nulos ni nan\n",
    "cantnn_muestra=dfspark_sample.count()\n",
    "print(\"cantidad de valores nulos: \",cant_muestra-cantnn_muestra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n",
      "time: 10 ms (started: 2021-05-24 19:15:11 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 401 ms (started: 2021-05-24 19:15:11 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# numero de particiones en las que se dividio la data.\n",
    "dfspark_sample.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, fare_amount: double, pickup_datetime: string, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, passenger_count: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 114 ms (started: 2021-05-24 19:15:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#que la data persista en memoria, acelera algunos procesos.\n",
    "dfspark_sample.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 311 ms (started: 2021-05-24 19:15:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 59s (started: 2021-05-24 19:15:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#casteamos a pandas\n",
    "summary=dfspark_sample.describe([\"pickup_longitude\",\n",
    "                                 \"pickup_latitude\",\n",
    "                                 \"dropoff_longitude\",\n",
    "                                 \"dropoff_latitude\",\n",
    "                                 \"passenger_count\",\n",
    "                                 \"fare_amount\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary              object\n",
       "pickup_longitude     object\n",
       "pickup_latitude      object\n",
       "dropoff_longitude    object\n",
       "dropoff_latitude     object\n",
       "passenger_count      object\n",
       "fare_amount          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.65 ms (started: 2021-05-24 19:18:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Tipos de datos del summary\n",
    "summary.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>2219013</td>\n",
       "      <td>2219013</td>\n",
       "      <td>2219013</td>\n",
       "      <td>2219013</td>\n",
       "      <td>2219013</td>\n",
       "      <td>2219013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>-72.49893468358957</td>\n",
       "      <td>39.91588546786556</td>\n",
       "      <td>-72.49993181216463</td>\n",
       "      <td>39.90892687019288</td>\n",
       "      <td>1.6843168561878636</td>\n",
       "      <td>11.346452639078764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>12.292290172231276</td>\n",
       "      <td>9.366487712029613</td>\n",
       "      <td>11.8250760373813</td>\n",
       "      <td>9.140430689717409</td>\n",
       "      <td>1.343318841289577</td>\n",
       "      <td>9.833546482396764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-3019.803228</td>\n",
       "      <td>-3114.472102</td>\n",
       "      <td>-2260.620487</td>\n",
       "      <td>-3111.70716</td>\n",
       "      <td>0</td>\n",
       "      <td>-105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>3442.185068</td>\n",
       "      <td>3310.364462</td>\n",
       "      <td>3442.185068</td>\n",
       "      <td>3375.202165</td>\n",
       "      <td>208</td>\n",
       "      <td>1564.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary    pickup_longitude    pickup_latitude   dropoff_longitude  \\\n",
       "0   count             2219013            2219013             2219013   \n",
       "1    mean  -72.49893468358957  39.91588546786556  -72.49993181216463   \n",
       "2  stddev  12.292290172231276  9.366487712029613    11.8250760373813   \n",
       "3     min        -3019.803228       -3114.472102        -2260.620487   \n",
       "4     max         3442.185068        3310.364462         3442.185068   \n",
       "\n",
       "    dropoff_latitude     passenger_count         fare_amount  \n",
       "0            2219013             2219013             2219013  \n",
       "1  39.90892687019288  1.6843168561878636  11.346452639078764  \n",
       "2  9.140430689717409   1.343318841289577   9.833546482396764  \n",
       "3        -3111.70716                   0              -105.0  \n",
       "4        3375.202165                 208              1564.5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 161 ms (started: 2021-05-24 19:18:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|passenger_count|  count|\n",
      "+---------------+-------+\n",
      "|              1|1535287|\n",
      "|              6|  46771|\n",
      "|              3|  97350|\n",
      "|              5| 157035|\n",
      "|              4|  46910|\n",
      "|              7|      1|\n",
      "|              2| 327848|\n",
      "|              0|   7806|\n",
      "|            208|      5|\n",
      "+---------------+-------+\n",
      "\n",
      "time: 6.59 s (started: 2021-05-24 19:18:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Agrupación por cantidad de pasajeros.\n",
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observaciones :\n",
    "1. Valores de passenger_count imposibles , como 34,49,51,129,208.\n",
    "2. Precios demasiados elevados debido a la cantidad de pasajeros y negativo(imposible).\n",
    "3. cantidad de datos en la que el precio es menor a o igual a 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformación de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionar passenger_count de 0-9\n",
    "dfspark_sample = dfspark_sample.filter(\"passenger_count < 10\")\n",
    "#Selecionar fare_amount mayor a 0\n",
    "dfspark_sample = dfspark_sample.filter(\"fare_amount >= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_sample.groupBy(\"passenger_count\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos columnas de diferencias.\n",
    "from pyspark.sql.functions import abs\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_latitude\",\n",
    "                                           abs(dfspark_sample['dropoff_latitude']-dfspark_sample['pickup_latitude']))\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dif_longitude\",\n",
    "                                           abs(dfspark_sample['dropoff_longitude']-dfspark_sample['pickup_longitude']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear dos columnas día de la semana y hora del viaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones que me ayudarán en la transformación.\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import calendar\n",
    "def dia(dia):\n",
    "    if dia == 1:\n",
    "        return 'lunes'\n",
    "    if dia == 2:\n",
    "        return 'martes'\n",
    "    if dia == 3:\n",
    "        return 'miércoles'\n",
    "    if dia == 4:\n",
    "        return 'jueves'\n",
    "    if dia == 5:\n",
    "        return 'viernes'\n",
    "    if dia == 6:\n",
    "        return 'sábado'\n",
    "    if dia == 7:\n",
    "        return 'domingo'\n",
    "    if dia < 1 or dia > 7:\n",
    "        return \n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def dia_semana(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_fecha = \"%Y-%m-%d\"\n",
    "    dia_semana = datetime.isoweekday(datetime.strptime(fecha,formato_fecha))\n",
    "    return dia_semana\n",
    "\n",
    "def hora(row):\n",
    "    fecha , hora , utc = row.split(\" \")\n",
    "    formato_hora = \"%H:%M:%S\"\n",
    "    hora = datetime.strptime(hora,formato_hora).hour\n",
    "    return hora\n",
    "    \n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "# convirtiendo las funciones en funciones UDF\n",
    "udf_dia_semana= udf( lambda z : dia_semana(z))\n",
    "udf_hora= udf( lambda z : hora(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfspark_sample = dfspark_sample.withColumn('dia_semana', \n",
    "                                           udf_dia_semana(dfspark_sample['pickup_datetime'] )  )\n",
    "dfspark_sample = dfspark_sample.withColumn('hora', \n",
    "                                           udf_hora(dfspark_sample['pickup_datetime'] )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Castear dia de la semana y hora\n",
    "dfspark_sample = dfspark_sample.withColumn(\"dia_semana\",\n",
    "                                           dfspark_sample[\"dia_semana\"].cast(\"Integer\"))\n",
    "dfspark_sample = dfspark_sample.withColumn(\"hora\",\n",
    "                                           dfspark_sample[\"hora\"].cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observación: En este punto la data estaria totalmente ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadística de las nuevas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_summary=dfspark_sample.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = [\"pickup_longitude\",\n",
    "            \"pickup_latitude\",\n",
    "            \"dropoff_longitude\",\n",
    "            \"dropoff_latitude\",\n",
    "            \"passenger_count\",\n",
    "            \"fare_amount\",\n",
    "            \"dif_latitude\",\n",
    "            \"dif_longitude\",\n",
    "            \"dia_semana\",\n",
    "            \"hora\"]\n",
    "clean_summary[columnas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlación de los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_old=[\"pickup_longitude\",\n",
    "         \"pickup_latitude\",\n",
    "         \"dropoff_longitude\",\n",
    "         \"dropoff_latitude\",\n",
    "         \"passenger_count\"]\n",
    "col_new=[\"dif_latitude\",\n",
    "         \"dif_longitude\",\n",
    "         \"dia_semana\",\n",
    "         \"hora\"]\n",
    "col_pred = [\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación en un vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "inputCols = col_old+col_new+col_pred\n",
    "assembler = VectorAssembler( inputCols=inputCols, outputCol=\"col_corr\")\n",
    "dfspark_corr = assembler.transform(dfspark_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_corr=dfspark_corr.select('col_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estudiar la correlación\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "dfspark_corr= Correlation.corr(dfspark_corr, 'col_corr','pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "#type(dfspark_cor.collect()[0][0]) # denseMatrix\n",
    "# Pasamos la matrix como un array.\n",
    "array_corr=dfspark_corr.collect()[0][0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corr= pd.DataFrame(array_corr, columns=inputCols, index=inputCols)\n",
    "mask = ~(pdf_corr>-0.3) | ~(pdf_corr<0.3)\n",
    "round(pdf_corr,10).style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observación:\n",
    "###### La variable a predecir fare_amount itnee muy baja correlación con las demás variables. Su mayor correlación es con passenger_count y hora. Pero si observamos correlación entre las demás variables. Definitivamente no podemos utilizar un modelo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de la data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos toda la data a Pandas\n",
    "pandasData = dfspark_sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manera alternativa de pasar la data a Pandas\n",
    "# import pandas as pd\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# # Wrapper for seamless Spark's serialisation\n",
    "# def spark_to_pandas(spark_df: DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     PySpark toPandas realisation using mapPartitions\n",
    "#     much faster than vanilla version\n",
    "#     fork: https://gist.github.com/lucidyan/1e5d9e490a101cdc1c2ed901568e082b\n",
    "#     origin: https://gist.github.com/joshlk/871d58e01417478176e7\n",
    "#     :param spark_df:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def _map_to_pandas(rdds) -> list:\n",
    "#         \"\"\" Needs to be here due to pickling issues \"\"\"\n",
    "#         return [pd.DataFrame(list(rdds))]\n",
    "\n",
    "#     def _to_pandas(df: DataFrame, n_partitions: int = None) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Returns the contents of `df` as a local `pandas.DataFrame` in a speedy fashion. The DataFrame is\n",
    "#         repartitioned if `n_partitions` is passed.\n",
    "#         :param df:\n",
    "#         :param n_partitions:\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         if n_partitions is not None:\n",
    "#             df = df.repartition(n_partitions)\n",
    "#         df_pand = df.rdd.mapPartitions(_map_to_pandas).collect()  # type: pd.DataFrame\n",
    "#         df_pand = pd.concat(df_pand)\n",
    "#         df_pand.columns = df.columns\n",
    "#         return df_pand\n",
    "\n",
    "#     return _to_pandas(spark_df)\n",
    "\n",
    "# pandasData = spark_to_pandas(dfspark_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostrando la data\n",
    "display(pandasData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\" , color_codes = True)\n",
    "var = [\"dia_semana\"]\n",
    "# 1 inches = 96px\n",
    "g = sns.pairplot(pandasData[:10], vars=var, diag_kind=\"hist\", hue='dia_semana', height=4, aspect=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuanto de la variable objetivo va variando segun los dias de la semana\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "carac1=pandasData['dia_semana']\n",
    "objet=pandasData['fare_amount']\n",
    "plt.plot(carac1, objet, 'o')\n",
    "plt.xlabel(\"Característica dia de la semana\")\n",
    "plt.ylabel(\"Objetivo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#como los pasajeros se distribuyen a traves de las horas\n",
    "pandasData.groupby('hora')['passenger_count'].sum().plot(kind='barh',legend='Reverse',figsize=(10,10))\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#como se reparten los pasajeros en funcion de la hora\n",
    "pandasData.passenger_count.groupby(pandasData.hora).sum().plot(kind='pie',cmap='Paired',figsize=(12,8))\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dispersion de las características\n",
    "caracteristicas=pandasData[['fare_amount','pickup_longitude','pickup_latitude','dropoff_longitude']]\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "g= sns.pairplot(caracteristicas,hue='fare_amount',palette='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 231 ms (started: 2021-05-24 19:18:18 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# cerramos la sesión spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}