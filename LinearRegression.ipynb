{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358c12c5",
   "metadata": {},
   "source": [
    "### Modelo de Regressión Lineal con SparkML y Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142ea4a",
   "metadata": {},
   "source": [
    "La regresión lineal es de aprendizaje supervisado y se utiliza en análisis de tendencias, análisis de series de tiempo, riesgo en la banca y muchos más. Para nuestro caso, lo usaremos para predecir el costo de un viaje en taxi.\n",
    "\n",
    "En una regresión lineal, una relación entre una variable dependiente y un conjunto de datos es lineal. Esto básicamente significa que si hay datos de una tendencia específica, se puede predecir una tendencia futura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d892a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-06-14 12:53:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# medir tiempos\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e464e8",
   "metadata": {},
   "source": [
    "#### Iniciando un entorno de trabajo con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d445930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-06-14 12:53:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "#findspark.init(\"/usr/local/spark/spark-3.1.1-bin-hadoop2.7\")  #para linux\n",
    "findspark.init()                                               #para windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ad44c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.19:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LinearRegression</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=LinearRegression>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.01 s (started: 2021-06-14 12:53:53 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# Variable de configuración\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"LinearRegression\")  \\\n",
    "    .set(\"spark.driver.maxResultSize\",\"0\") \\\n",
    "    .set(\"spark.driver.memory\", \"8g\") \\\n",
    "    .set(\"spark.cores.max\", \"8\")\n",
    "    #.set(\"spark.executor.memory\", \"1g\") \\\n",
    "    \n",
    "# Iniciamos un contexto spark (solo se ejecuta uno. Para ejecutar otra vez , reiniciar el kernel)\n",
    "sc = SparkContext(conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be90137",
   "metadata": {},
   "source": [
    "#### Cargando el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db6c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.17 s (started: 2021-06-14 12:53:57 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "# le pasamos el contexto anterior\n",
    "sqlContext = SQLContext(sc)\n",
    "import os\n",
    "# path=\"file:\"+os.getcwd()+\"/Output\"           # para linux\n",
    "path=os.getcwd()+\"/Output\"                     # para windows\n",
    "dfspark = sqlContext.read.format('csv').option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(path+'/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ebbd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- dif_latitude: double (nullable = true)\n",
      " |-- dif_longitude: double (nullable = true)\n",
      " |-- distancia: double (nullable = true)\n",
      " |-- dia_semana: integer (nullable = true)\n",
      " |-- hora: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      "\n",
      "time: 16 ms (started: 2021-06-14 12:54:05 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13774578",
   "metadata": {},
   "source": [
    "La característica pickup_datetime ya se usó para crear nuevos datos, por lo que procederemos a eliminarla antes de construir el modelo de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa94cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-06-14 12:54:05 -05:00)\n"
     ]
    }
   ],
   "source": [
    "dfspark = dfspark.drop('pickup_datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d86ea",
   "metadata": {},
   "source": [
    "Preparando el conjunto de datos para el aprensizaje automático. Sólo se usará dos columnas, la etiqueta (fare_amount) que es valor a predecir, y otra columna de características. Para ello se importa VectoAssembler que transformará los datos en un vector de carácterísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ac03fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|fare_amount|\n",
      "+--------------------+-----------+\n",
      "|[-73.961407,40.71...|       13.0|\n",
      "|[-74.011287,40.70...|       45.0|\n",
      "|[-73.991907,40.73...|       16.9|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "time: 578 ms (started: 2021-06-14 12:54:05 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','distancia', 'dia_semana', 'hora', 'mes', 'anio','passenger_count'], outputCol = 'features')\n",
    "dfspark = vectorAssembler.transform(dfspark)\n",
    "dfspark = dfspark.select(['features', 'fare_amount'])\n",
    "dfspark.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e6df6",
   "metadata": {},
   "source": [
    "División del conjunto de datos usando randomsplit, 80% para entrenamiento y 20% para prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09fde275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms (started: 2021-06-14 12:54:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "splits = dfspark.randomSplit([0.8, 0.2],seed=0)\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea317cb",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657f0c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [0.0,0.0,0.0,0.0,0.00038516866757571153,0.0,-0.012793274508220114,0.07589766478781158,0.5461330399276331,0.03932990644267434]\n",
      "Intercepto: -1087.7769061927456\n",
      "time: 6.8 s (started: 2021-06-14 12:54:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='fare_amount', maxIter=30, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(dfspark)\n",
    "print(\"Coeficientes: \" + str(lr_model.coefficients))\n",
    "print(\"Intercepto: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46280d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.319407\n",
      "r2: 0.015916\n",
      "time: 0 ns (started: 2021-06-14 12:54:21 -05:00)\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43771847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|11.282150381536894|       16.9|(10,[5,6,7,8,9],[...|\n",
      "|10.811915006397157|       12.9|(10,[5,6,7,8,9],[...|\n",
      "|10.177091027173446|       45.0|(10,[5,6,7,8,9],[...|\n",
      "|10.723224067101228|        5.7|(10,[5,6,7,8,9],[...|\n",
      "|10.723224067101228|       13.3|(10,[5,6,7,8,9],[...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R al cuadrado (R2) en datos de prueba = 0.015516\n",
      "time: 7.58 s (started: 2021-06-14 12:54:39 -05:00)\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"fare_amount\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"fare_amount\",metricName=\"r2\")\n",
    "print(\"R al cuadrado (R2) en datos de prueba = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b23287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio (RMSE) en los datos de prueba = 9.33447\n",
      "time: 3.42 s (started: 2021-06-14 12:59:27 -05:00)\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Error cuadrático medio (RMSE) en los datos de prueba = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fd0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
